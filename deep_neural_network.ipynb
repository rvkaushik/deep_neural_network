{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    file_name=r\"C:\\Users\\Ravi\\Anaconda3\\my_learning\\datasets\\train_catvnoncat.h5\"\n",
    "    file_name_test=r\"C:\\Users\\Ravi\\Anaconda3\\my_learning\\datasets\\test_catvnoncat.h5\"\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(file_name_test, \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x),x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x)),x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parametres(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) *0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(A, W, b):\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_activation_based(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = forward_propagation(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = forward_propagation(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  \n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_propagation_activation_based(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],\n",
    "                                             activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = forward_propagation_activation_based(A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert (AL.shape == (1, X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (1. / m) * (-np.dot(Y, np.log(AL).T) - np.dot(1 - Y, np.log(1 - AL).T)) #cost_function\n",
    "\n",
    "    cost = np.squeeze(cost) \n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(dZ, cache):\n",
    "   \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1. / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1. / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_activation_based(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = backward_propagation(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = backward_propagation(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_propagation_activation_based(dAL, current_cache,\n",
    "                                                                                                  activation=\"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_propagation_activation_based(grads[\"dA\" + str(l + 2)], current_cache,\n",
    "                                                                    activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2  # number of layers in the neural network\n",
    "    p = np.zeros((1, m))\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.75:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "\n",
    "    # print results\n",
    "    print(\"Accuracy: \" + str(np.sum((p == y) / (m*1.0))))\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    parameters = initialise_parametres(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "    \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.678000\n",
      "Cost after iteration 200: 0.667584\n",
      "Cost after iteration 300: 0.660405\n",
      "Cost after iteration 400: 0.655441\n",
      "Cost after iteration 500: 0.651999\n",
      "Cost after iteration 600: 0.649603\n",
      "Cost after iteration 700: 0.647931\n",
      "Cost after iteration 800: 0.646761\n",
      "Cost after iteration 900: 0.645940\n",
      "Cost after iteration 1000: 0.645362\n",
      "Cost after iteration 1100: 0.644956\n",
      "Cost after iteration 1200: 0.644669\n",
      "Cost after iteration 1300: 0.644466\n",
      "Cost after iteration 1400: 0.644323\n",
      "Cost after iteration 1500: 0.644222\n",
      "Cost after iteration 1600: 0.644150\n",
      "Cost after iteration 1700: 0.644099\n",
      "Cost after iteration 1800: 0.644062\n",
      "Cost after iteration 1900: 0.644037\n",
      "Cost after iteration 2000: 0.644018\n",
      "Cost after iteration 2100: 0.644006\n",
      "Cost after iteration 2200: 0.643996\n",
      "Cost after iteration 2300: 0.643990\n",
      "Cost after iteration 2400: 0.643985\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, [12288, 20, 7, 5, 1], num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555023923444976\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parametres(list_of_hidden):\n",
    "    w=[]\n",
    "    b=[]\n",
    "    for i in range(1,len(list_of_hidden)):\n",
    "        w.append(np.random.randn(list_of_hidden[i],list_of_hidden[i-1]))\n",
    "        b.append(np.zeros((list_of_hidden[i],1)))\n",
    "    parameters={\n",
    "        \"w\":w,\n",
    "        \"b\":b\n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(parameters,x,y):\n",
    "    z=[]\n",
    "    a=[]\n",
    "    x_prev=x\n",
    "    w=parameters[\"w\"]\n",
    "    b=parameters[\"b\"]\n",
    "    for i in range(0,len(w)-1):\n",
    "        z.append(np.dot(w[i],x_prev)+b[i])\n",
    "        x_prev=relu(z[i])\n",
    "        a.append(x_prev)\n",
    "    x_prev=a[-1]\n",
    "    z.append(np.dot(w[len(w)-1],x_prev)+b[len(w)-1])\n",
    "    a.append(sigmoid(z[-1]))\n",
    "    return z,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a,Y):\n",
    "    m = Y.shape[1]\n",
    "    print (m)\n",
    "    cost = -(np.dot(Y,np.log(a[-1].T))+np.dot(1-Y,np.log(1-a[-1].T)))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    print (cost)\n",
    "    print(len(a[-1][0]))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=initialise_parametres([len(train_x),2,4,1])\n",
    "z,a=forward_propagate(parameters,train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "1.3957865281680026\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "cost=compute_cost(a,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(a,z,y,parameters):\n",
    "    print (len(z))\n",
    "    m=len(z[0][0])\n",
    "    w=parameters[\"w\"]\n",
    "    dw,da,db=[],[],[]\n",
    "    da.append(- (np.divide(y, a[-1]) - np.divide(1 - y, 1 - a[-1])))\n",
    "    dz=[]\n",
    "    dz.append(sigmoid_backward(da[0], z[-1]))\n",
    "    db.append( 1. / m * np.sum(dz[0], axis=1, keepdims=True))\n",
    "    dw.append(1. / m * np.dot(dz[0], a[-1].T))\n",
    "    print (dw)\n",
    "    for l in range(len(z)-1, 0, -1):\n",
    "        print (l)\n",
    "        da.append(np.dot(w[l-1].T, dz[len(z)-l-1]))\n",
    "        dz.append(relu_backward(da[len(z)-l-1], z[l]))\n",
    "        db.append( 1. / m * np.sum(dz[len(z)-l], axis=1, keepdims=True))\n",
    "        dw.append(1. / m * np.dot(dz[len(z)-l], a[l].T))\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[array([[0.07479369]])]\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,4) and (1,209) not aligned: 4 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-bcd819f3dd37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-239-aeafed79abbd>\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(a, z, y, parameters)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,4) and (1,209) not aligned: 4 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "backward_prop(a,z,train_y,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1. / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1. / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
